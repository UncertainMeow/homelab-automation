Directory structure:
‚îî‚îÄ‚îÄ uncertainmeow-infra-snapshot/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ docs/
    ‚îÇ   ‚îú‚îÄ‚îÄ CUSTOMIZATION.md
    ‚îÇ   ‚îî‚îÄ‚îÄ QUICKSTART.md
    ‚îú‚îÄ‚îÄ examples/
    ‚îÇ   ‚îî‚îÄ‚îÄ AGENTS.md.example
    ‚îú‚îÄ‚îÄ playbooks/
    ‚îÇ   ‚îú‚îÄ‚îÄ inventory.example.ini
    ‚îÇ   ‚îú‚îÄ‚îÄ playbook-baseball-card.yml
    ‚îÇ   ‚îî‚îÄ‚îÄ playbook-work-friend.yml
    ‚îî‚îÄ‚îÄ templates/
        ‚îú‚îÄ‚îÄ baseball-card.md.j2
        ‚îî‚îÄ‚îÄ work-friend.md.j2

================================================
FILE: README.md
================================================
# Infra Snapshot

**Stop hand-waving. Start snapshotting.** üì∏

> *"Give your AI agents actual facts instead of embarrassingly vague descriptions of your homelab"*

[![Ansible](https://img.shields.io/badge/ansible-2.9+-blue.svg)](https://www.ansible.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Automated infrastructure snapshots at multiple detail levels.**

---

## üéØ The Problem

**Before:**
```
You: "I have like... 3 or 4 Proxmox hosts? Maybe 5?
     One's a Dell with GPUs I think... or wait, is it 2 GPUs?
     It runs... stuff? Docker? LXC? Both?"

AI: "Based on your vague hand-waving..."
You: *dies inside*
```

**After:**
```
You: "See AGENTS.md for my current infrastructure"

AI: [reads file]
    "I see you have socrates (Dell R730, 24 vCPU, 128GB RAM,
     2x NVIDIA GPUs, currently at 64% RAM utilization, running
     Ollama and OpenWebUI containers, uptime 3 days 14 hours).

     For your new GitLab deployment, I recommend rawls
     (32GB RAM, only 25% utilized, low CPU load)..."

You: *chef's kiss*
```

---

## üÉè Snapshot Levels

### Level 1: Baseball Card ‚öæ

**Quick essential facts** - Perfect for AI agents

**Includes:**
- Status (online/offline)
- IP address, hostname
- OS and version
- CPU/RAM basics
- Uptime
- What's running (containers, VMs)
- Role and purpose

**Output:** Concise markdown table format

**Time:** ~30 seconds

**Use for:** Updating AGENTS.md, quick status checks, "what do I have?"

### Level 2: Work Friend üëî

**Extended conversational detail** - Like chatting with a coworker

**Includes everything from Baseball Card PLUS:**
- Detailed disk usage (all mounts)
- Top 10 largest directories
- Storage pools (ZFS, LVM)
- Network interfaces and connections
- Listening services with ports
- Top processes (CPU and memory)
- Recent log entries (last 50)
- Error/warning counts
- Failed services
- Temperature readings
- Docker container details
- Proxmox cluster status
- Available updates

**Output:** Detailed conversational markdown

**Time:** ~2-3 minutes

**Use for:** Troubleshooting, capacity planning, deep dives

---

## üöÄ Quick Start

### Installation

```bash
# Clone the repo
git clone https://github.com/yourusername/infra-snapshot.git
cd infra-snapshot

# Install Ansible
brew install ansible  # macOS
# or
apt install ansible   # Ubuntu/Debian

# Update inventory with your hosts
vim playbooks/inventory.example.ini
mv playbooks/inventory.example.ini playbooks/inventory.ini

# Test connectivity
ansible -i playbooks/inventory.ini ssh_accessible -m ping

# Run Level 1 snapshot (Baseball Card)
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml

# Check output
cat snapshots/baseball-card-$(date +%Y-%m-%d).md

# Run Level 2 snapshot (Work Friend)
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-work-friend.yml

# Check output
cat snapshots/work-friend-$(date +%Y-%m-%d).md
```

---

## üèóÔ∏è How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your Infrastructure‚îÇ
‚îÇ  ‚Ä¢ Proxmox hosts    ‚îÇ
‚îÇ  ‚Ä¢ VMs & containers ‚îÇ
‚îÇ  ‚Ä¢ Storage servers  ‚îÇ
‚îÇ  ‚Ä¢ Network devices  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Ansible queries via SSH
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ansible Playbook   ‚îÇ
‚îÇ  ‚Ä¢ Gather facts     ‚îÇ
‚îÇ  ‚Ä¢ Run commands     ‚îÇ
‚îÇ  ‚Ä¢ Check services   ‚îÇ
‚îÇ  ‚Ä¢ Read logs        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Renders template
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Markdown Output    ‚îÇ
‚îÇ  ‚Ä¢ Baseball Card    ‚îÇ
‚îÇ  ‚Ä¢ Work Friend      ‚îÇ
‚îÇ  ‚Ä¢ AGENTS.md        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Copy to
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Agent Context   ‚îÇ
‚îÇ  "See AGENTS.md"    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### The Magic

1. **Ansible connects** to your infrastructure via SSH
2. **Gathers facts** (OS, CPU, RAM, uptime, etc.)
3. **Runs commands** to get additional data
4. **Renders Jinja2 template** with all the data
5. **Outputs markdown** perfect for AI agents

**No manual documentation!** Just run a command, copy to AGENTS.md, done.

---

## üìä Example Output

### Baseball Card Level

```markdown
## üñ•Ô∏è socrates

**Status:** ‚úÖ Online
**IP Address:** 10.203.3.42
**Role:** compute
**OS:** Proxmox VE 8.1
**Hostname:** socrates
**Architecture:** x86_64
**Uptime:** up 3 days, 14 hours
**CPU:** 24 vCPUs
**RAM:** 128.0 GB (64.5/128 GB used)
**Disk Usage:** 45%
**Type:** Proxmox Host
**Proxmox Version:** pve-manager/8.1.3
**LXC Containers:** ollama, openwebui
**Description:** Dell R730 with 2x GPUs for AI workloads
**Notes:** Requires fan control container
```

### Work Friend Level

```markdown
# üñ•Ô∏è SOCRATES

## Basic Info
- **Status:** ‚úÖ Online
- **IP Address:** 10.203.3.42
- **Load Average:** 2.14, 1.89, 1.75
...

## Storage
```
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       100G   45G   55G  45% /
/dev/sdb1       500G  320G  180G  64% /var/lib/vz
```

## Performance
### Top 5 Processes by CPU
```
USER       PID %CPU %MEM COMMAND
root      1234  45.2  12.1 ollama serve
```

## Recent Logs (Last 50)
```
Dec 05 10:23:15 socrates systemd[1]: Started Ollama LLM Service
```
```

---

## üéØ Use Cases

### 1. AI Agent Context

**Problem:** AI doesn't know what you have
**Solution:** Give it AGENTS.md with current snapshot

```bash
# Before talking to AI
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml
# Copy relevant sections to AGENTS.md
# Start AI conversation with "See AGENTS.md"
```

### 2. Capacity Planning

**Problem:** Which host has room for new service?
**Solution:** Work Friend level snapshot shows resource usage

```bash
# Run detailed snapshot
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-work-friend.yml
# Review RAM/CPU/disk usage
# Deploy to host with capacity
```

### 3. Troubleshooting

**Problem:** Something's wrong but don't know where
**Solution:** Work Friend shows errors, failed services, logs

```bash
# Run detailed snapshot
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-work-friend.yml
# Review "Health & Logs" sections
# See failed services, error counts, recent logs
```

### 4. Change Tracking

**Problem:** What changed since last week?
**Solution:** Commit snapshots to git, compare

```bash
# Snapshot before change
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml
git add snapshots/ && git commit -m "Pre-deployment snapshot"

# Make changes...

# Snapshot after
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml
git diff HEAD snapshots/baseball-card-*.md
```

### 5. Documentation

**Problem:** Infrastructure docs always out of date
**Solution:** Re-snapshot regularly, auto-update docs

```bash
# Cron job runs weekly
0 2 * * 0 cd /path/to/repo && ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml
```

---

## üìñ Documentation

- [Quick Start Guide](docs/QUICKSTART.md) - Get up and running
- [Customization Guide](docs/CUSTOMIZATION.md) - Add your own checks
- [AI Agents Guide](docs/AI-AGENTS-GUIDE.md) - Best practices for AI context
- [Scheduling Guide](docs/SCHEDULING.md) - Automate with cron/systemd

---

## üõ†Ô∏è Configuration

### Inventory File

Define your infrastructure in `playbooks/inventory.ini`:

```ini
[proxmox_hosts]
socrates ansible_host=10.203.3.42 ansible_user=root role=compute
rawls ansible_host=10.203.3.47 ansible_user=root role=general

[storage]
stuffs ansible_host=10.203.3.99 ansible_user=kellen ansible_become=yes role=nas

[production_services]
gitlab ansible_host=10.203.4.15 ansible_user=root role=iac

[all:vars]
ansible_python_interpreter=/usr/bin/python3
```

### Custom Checks

Add your own checks by editing playbooks:

```yaml
- name: Check custom service
  ansible.builtin.shell: systemctl status my-service
  register: my_service_status
  changed_when: false
  failed_when: false
```

Then reference in template:

```jinja
{% if facts.my_service_status is defined %}
**My Service:** {{ facts.my_service_status.stdout }}
{% endif %}
```

---

## üé® Customization

### Change Output Format

Templates use Jinja2 - easy to customize:

```jinja
{# Add emoji indicators #}
**Status:** {% if ping_result.ping is defined %}‚úÖ Online{% else %}‚ùå Offline{% endif %}

{# Color code disk usage #}
**Disk:** {% if disk_usage.stdout | int > 90 %}üî¥{% elif disk_usage.stdout | int > 70 %}üü°{% else %}üü¢{% endif %} {{ disk_usage.stdout }}

{# Group by role #}
{% for role in ['compute', 'storage', 'networking'] %}
## {{ role | upper }} Hosts
{% for host in groups['ssh_accessible'] if hostvars[host].role == role %}
...
{% endfor %}
{% endfor %}
```

### Filter Hosts

Run snapshot on specific hosts:

```bash
# Only Proxmox hosts
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml --limit proxmox_hosts

# Specific host
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml --limit socrates
```

---

## üìÖ Scheduling

### Cron

```bash
# Daily baseball card at 2 AM
0 2 * * * cd /path/to/repo && ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml

# Weekly work friend on Sundays
0 3 * * 0 cd /path/to/repo && ansible-playbook -i playbooks/inventory.ini playbooks/playbook-work-friend.yml
```

### Systemd Timer

```bash
# /etc/systemd/system/homelab-snapshot.service
[Unit]
Description=Homelab Snapshot

[Service]
Type=oneshot
WorkingDirectory=/path/to/repo
ExecStart=/usr/bin/ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml

# /etc/systemd/system/homelab-snapshot.timer
[Unit]
Description=Daily Snapshot

[Timer]
OnCalendar=daily

[Install]
WantedBy=timers.target

# Enable
systemctl enable --now homelab-snapshot.timer
```

---

## üêõ Troubleshooting

### Host Unreachable

```bash
# Test SSH manually
ssh root@10.203.3.42

# Test with Ansible
ansible -i playbooks/inventory.ini socrates -m ping

# Check inventory
ansible-inventory -i playbooks/inventory.ini --list
```

### Permission Denied

Add `ansible_become=yes` to inventory:

```ini
stuffs ansible_host=10.203.3.99 ansible_user=kellen ansible_become=yes
```

### Slow Execution

Run in parallel:

```bash
ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml --forks=10
```

---

## üí° Pro Tips

### Quick Access Aliases

Add to `~/.zshrc` or `~/.bashrc`:

```bash
alias hl-snap='cd /path/to/repo && ansible-playbook -i playbooks/inventory.ini playbooks/playbook-baseball-card.yml'
alias hl-deep='cd /path/to/repo && ansible-playbook -i playbooks/inventory.ini playbooks/playbook-work-friend.yml'
alias hl-show='cat /path/to/repo/snapshots/baseball-card-$(date +%Y-%m-%d).md'
```

### Version Control Snapshots

```bash
# Commit snapshots for history
git add snapshots/
git commit -m "Snapshot $(date +%Y-%m-%d)"

# Compare over time
git log --oneline snapshots/
git diff HEAD~7 snapshots/baseball-card-*.md
```

### JSON Output

Export as JSON for automation:

```yaml
- name: Save as JSON
  ansible.builtin.copy:
    content: "{{ hostvars | to_nice_json }}"
    dest: "./snapshots/snapshot-{{ ansible_date_time.date }}.json"
```

---

## üìú License

MIT License - see [LICENSE](LICENSE) file for details

---

## üôè Acknowledgments

- Built for homelabbers tired of saying "I think I have..."
- Inspired by actual embarrassing conversations with AI agents
- Named "Baseball Cards" because that's what they are
- No more hand-waving, just facts

---

## üé§ Author

Built with ‚òï and ü§ñ by Kellen & Claude

**Status:** Production-ready (used in real homelab since 2025-12-05)

**Philosophy:** "If you can't measure it, you can't manage it. If you can't snapshot it, you can't describe it to AI without looking silly."

---

*"Stop hand-waving. Start snapshotting."* üì∏



================================================
FILE: docs/CUSTOMIZATION.md
================================================
[Empty file]


================================================
FILE: docs/QUICKSTART.md
================================================
# Homelab Snapshot System

**Purpose:** Generate infrastructure snapshots at varying detail levels for AI agents and documentation.

**Philosophy:** Stop waving your hands when describing your homelab to AI agents. Give them actual, current data.

---

## üìä Snapshot Levels

### Level 0: Existence ‚ùå
**"You exist at the party but you're about to throw up"**

This level is too basic - just knowing something exists isn't helpful. **We don't support this level.**

### Level 1: Baseball Card ‚úÖ
**"Quick stats on the back of a baseball card"**

**Perfect for:** Dropping into AGENTS.md for AI conversations
**Time to run:** ~30 seconds
**Detail level:** Essential facts only

**Includes:**
- Host status (online/offline)
- IP address, hostname
- OS and version
- CPU/RAM basics
- Uptime
- What's running (containers, VMs)
- Role and purpose

**Output:** Concise markdown table format

**Run:**
```bash
ansible-playbook -i inventory.ini playbook-baseball-card.yml
```

**Example output snippet:**
```markdown
## üñ•Ô∏è socrates

**Status:** ‚úÖ Online
**IP Address:** 10.203.3.42
**Role:** compute
**OS:** Proxmox VE 8.1
**Uptime:** up 3 days, 14 hours
**CPU:** 24 vCPUs
**RAM:** 128.0 GB (64.5/128 GB used)
**LXC Containers:** ollama, openwebui
**Description:** Dell R730 with 2x GPUs for AI workloads
```

### Level 2: Work Friend ‚úÖ
**"Chatting with a coworker about infrastructure"**

**Perfect for:** Deep dives, troubleshooting, planning
**Time to run:** ~2-3 minutes
**Detail level:** Extended information

**Includes everything from Baseball Card PLUS:**
- Detailed disk usage (all mounts)
- Top 10 largest directories
- Storage pools (ZFS, LVM)
- Network interfaces and active connections
- Listening services with ports
- Top processes (CPU and memory)
- Recent log entries (last 50)
- Error/warning counts
- Failed services
- Temperature readings (if available)
- Docker container details
- Proxmox cluster status
- Available updates

**Output:** Conversational markdown format with context

**Run:**
```bash
ansible-playbook -i inventory.ini playbook-work-friend.yml
```

**Example output snippet:**
```markdown
# üñ•Ô∏è SOCRATES

## Basic Info
- **Status:** ‚úÖ Online
- **IP Address:** 10.203.3.42
...

## Storage
```
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       100G   45G   55G  45% /
/dev/sdb1       500G  320G  180G  64% /var/lib/vz
```

### Top 10 Largest Directories
```
45G     /var/lib/vz/images
12G     /var/log
8G      /opt/ollama
...
```

## Performance
### Top 5 Processes by CPU
```
USER       PID %CPU %MEM COMMAND
root      1234  45.2  12.1 ollama serve
...
```

## Recent Logs
```
Dec 05 10:23:15 socrates systemd[1]: Started Ollama LLM Service
Dec 05 10:23:20 socrates kernel: NVIDIA GPU initialized
...
```
```

### Level 3: Oppo Research ‚ùå
**"Every detail for competitive intelligence"**

This level would include:
- Complete log analysis
- Security audit trails
- Configuration file dumps
- Network traffic analysis
- Performance benchmarking

**We don't support this level.** Too much detail, too slow, and usually not needed.

---

## üéØ Which Level When?

### Use Baseball Card When:
- Starting a conversation with an AI agent
- Quick status check
- Updating AGENTS.md
- Giving someone a tour of your lab
- Creating a "what do I have?" reference

### Use Work Friend When:
- Troubleshooting performance issues
- Planning capacity expansion
- Investigating errors or failures
- Deep-dive with AI on specific problems
- Before/after major changes
- Documentation for compliance or handoff

---

## üìÅ Files

### Ansible Inventory
**`inventory.ini`** - Defines all infrastructure hosts and groups

**Key groups:**
- `proxmox_hosts` - Hypervisors
- `storage` - NAS and file servers
- `production_services` - LXC/VMs running services
- `ssh_accessible` - All hosts you can SSH to

**Update this file** when adding/removing hosts.

### Playbooks

**`playbook-baseball-card.yml`** - Level 1 snapshot
- Pings all hosts
- Gathers essential facts
- Generates concise markdown report

**`playbook-work-friend.yml`** - Level 2 snapshot
- Extended fact gathering
- Queries logs, processes, storage
- Generates detailed markdown report

### Templates

**`templates/baseball-card.md.j2`** - Jinja2 template for Level 1 output
**`templates/work-friend.md.j2`** - Jinja2 template for Level 2 output

Edit these to customize output format.

### Output

**`snapshots/baseball-card-YYYY-MM-DD.md`** - Generated Level 1 reports
**`snapshots/work-friend-YYYY-MM-DD.md`** - Generated Level 2 reports

Timestamped so you can track changes over time.

---

## üöÄ Quick Start

### Prerequisites

```bash
# Install Ansible
brew install ansible  # macOS
# or
apt install ansible   # Ubuntu/Debian

# Verify
ansible --version
```

### First Run

```bash
# 1. Update inventory with your hosts
vim inventory.ini

# 2. Test connectivity
ansible -i inventory.ini ssh_accessible -m ping

# 3. Run baseball card snapshot
ansible-playbook -i inventory.ini playbook-baseball-card.yml

# 4. View output
cat snapshots/baseball-card-$(date +%Y-%m-%d).md
```

### Update AGENTS.md

```bash
# 1. Run snapshot
ansible-playbook -i inventory.ini playbook-baseball-card.yml

# 2. Copy relevant sections to AGENTS.md
# (Manual step - copy the host info you want)
```

---

## üîß Customization

### Add Custom Facts

Edit the playbook to add your own tasks:

```yaml
- name: Check custom service
  ansible.builtin.shell: systemctl status my-service
  register: my_service_status
  changed_when: false
  failed_when: false
```

Then reference in the template:

```jinja
{% if facts.my_service_status is defined %}
**My Service Status:** {{ facts.my_service_status.stdout }}
{% endif %}
```

### Change Output Format

Templates use Jinja2. Common changes:

**Add emoji status indicators:**
```jinja
**Status:** {% if ping_result.ping is defined %}‚úÖ Online{% else %}‚ùå Offline{% endif %}
```

**Format disk usage with colors (for terminal output):**
```jinja
**Disk Usage:** {% if disk_usage.stdout | replace('%','') | int > 90 %}üî¥{% elif disk_usage.stdout | replace('%','') | int > 70 %}üü°{% else %}üü¢{% endif %} {{ disk_usage.stdout }}
```

**Group hosts by role:**
```jinja
{% for role in ['compute', 'storage', 'networking'] %}
## {{ role | upper }} Hosts
{% for host in groups['ssh_accessible'] %}
{% if hostvars[host].role == role %}
...
{% endif %}
{% endfor %}
{% endfor %}
```

### Filter Hosts

Run snapshot on specific hosts only:

```bash
# Only Proxmox hosts
ansible-playbook -i inventory.ini playbook-baseball-card.yml --limit proxmox_hosts

# Only production services
ansible-playbook -i inventory.ini playbook-baseball-card.yml --limit production_services

# Specific host
ansible-playbook -i inventory.ini playbook-baseball-card.yml --limit socrates
```

### Change Output Location

Edit playbook:

```yaml
- name: Generate baseball card markdown
  ansible.builtin.template:
    src: templates/baseball-card.md.j2
    dest: "/path/to/custom/location/snapshot.md"
```

---

## üìÖ Scheduling

### Cron (Manual)

```bash
# Run baseball card snapshot daily at 2 AM
0 2 * * * cd /path/to/repo && ansible-playbook -i inventory.ini playbook-baseball-card.yml

# Run work friend snapshot weekly on Sunday at 3 AM
0 3 * * 0 cd /path/to/repo && ansible-playbook -i inventory.ini playbook-work-friend.yml
```

### Systemd Timer

```bash
# /etc/systemd/system/homelab-snapshot.service
[Unit]
Description=Homelab Baseball Card Snapshot

[Service]
Type=oneshot
WorkingDirectory=/path/to/repo
ExecStart=/usr/bin/ansible-playbook -i inventory.ini playbook-baseball-card.yml

# /etc/systemd/system/homelab-snapshot.timer
[Unit]
Description=Daily Homelab Snapshot

[Timer]
OnCalendar=daily
Persistent=true

[Install]
WantedBy=timers.target

# Enable
systemctl enable --now homelab-snapshot.timer
```

---

## üêõ Troubleshooting

### Host Unreachable

```bash
# Test SSH manually
ssh root@10.203.3.42

# Check inventory
ansible-inventory -i inventory.ini --list

# Test ping
ansible -i inventory.ini socrates -m ping
```

**Common issues:**
- SSH key not configured (use `ansible_ssh_pass` or `ssh-copy-id`)
- Host is down
- Firewall blocking SSH
- Wrong IP in inventory

### Permission Denied

**Solution 1:** Use `ansible_become=yes` in inventory:
```ini
stuffs ansible_host=10.203.3.99 ansible_user=kellen ansible_become=yes
```

**Solution 2:** Add to playbook:
```yaml
- hosts: all
  become: yes
```

### Command Not Found

Some commands may not exist on all systems:

```yaml
- name: Get uptime (fallback)
  ansible.builtin.shell: uptime -p || uptime
  register: uptime_output
```

### Slow Execution

**Use --forks to run in parallel:**
```bash
ansible-playbook -i inventory.ini playbook-baseball-card.yml --forks=10
```

**Disable fact gathering for faster runs:**
```yaml
gather_facts: no
```

(But then you won't get as much detail!)

---

## üí° Pro Tips

### 1. Version Control Snapshots

```bash
# Commit snapshots to git
git add snapshots/
git commit -m "Snapshot $(date +%Y-%m-%d): Post network reset"
git push
```

Now you have historical snapshots!

### 2. Compare Snapshots

```bash
# See what changed
diff snapshots/baseball-card-2025-12-01.md snapshots/baseball-card-2025-12-05.md
```

### 3. JSON Output for Automation

Modify playbook to also output JSON:

```yaml
- name: Save as JSON
  ansible.builtin.copy:
    content: "{{ hostvars | to_nice_json }}"
    dest: "./snapshots/snapshot-{{ ansible_date_time.date }}.json"
```

Then parse with scripts:

```bash
jq '.socrates.ansible_facts.ansible_memtotal_mb' snapshots/snapshot-2025-12-05.json
```

### 4. Slack/Discord Notifications

Add to playbook:

```yaml
- name: Send to Slack
  ansible.builtin.uri:
    url: https://hooks.slack.com/services/YOUR/WEBHOOK/URL
    method: POST
    body_format: json
    body:
      text: "Homelab snapshot complete! {{ ansible_date_time.iso8601 }}"
```

### 5. Create Diff Reports

```bash
#!/bin/bash
# snapshot-diff.sh

YESTERDAY=$(date -d yesterday +%Y-%m-%d)
TODAY=$(date +%Y-%m-%d)

ansible-playbook -i inventory.ini playbook-baseball-card.yml

diff -u snapshots/baseball-card-$YESTERDAY.md snapshots/baseball-card-$TODAY.md > snapshots/diff-$TODAY.txt

echo "Changes since yesterday:" snapshots/diff-$TODAY.txt
```

---

## ü§ñ Using with AI Agents

### Include in Agent Context

**Option 1: Paste into conversation**
```
Here's my current homelab snapshot:
[paste baseball-card output]
```

**Option 2: Reference AGENTS.md**
```
See AGENTS.md for my current infrastructure
```

**Option 3: Provide on-demand**
```
AI: "What Proxmox hosts do you have?"
You: [run playbook, paste output]
```

### Update AGENTS.md Regularly

```bash
# Weekly update workflow
ansible-playbook -i inventory.ini playbook-baseball-card.yml
# Copy relevant sections to AGENTS.md
vim AGENTS.md
git commit -am "Update AGENTS.md with current state"
```

### AI-Friendly Output

The markdown format is designed to be AI-readable:
- Clear section headers
- Consistent formatting
- Key-value pairs
- Status indicators (‚úÖ/‚ùå)
- Contextual notes

AI agents can parse this easily and give accurate answers.

---

## üìä Example Workflow

**Scenario:** Planning to add a new service

1. **Generate current snapshot:**
   ```bash
   ansible-playbook -i inventory.ini playbook-work-friend.yml
   ```

2. **Review available resources:**
   - Check CPU/RAM usage
   - Check disk space
   - Check which hosts have capacity

3. **Ask AI with context:**
   ```
   Here's my current infrastructure: [paste snapshot]

   I want to deploy GitLab. Which host should I use?
   ```

4. **AI responds with data-driven recommendation:**
   ```
   Based on your snapshot:
   - socrates: 64.5/128 GB RAM used, high CPU (AI workloads)
   - rawls: 8/32 GB RAM used, low CPU ‚úÖ RECOMMENDED

   Deploy GitLab LXC on rawls at 10.203.4.15
   ```

5. **Update inventory and re-snapshot after deployment:**
   ```bash
   # Add to inventory.ini
   gitlab ansible_host=10.203.4.15 ...

   # Re-snapshot
   ansible-playbook -i inventory.ini playbook-baseball-card.yml

   # Update AGENTS.md
   ```

---

## üéâ You're Ready!

You now have a system to:
- ‚úÖ Generate infrastructure snapshots automatically
- ‚úÖ Choose detail level based on need
- ‚úÖ Give AI agents accurate, current data
- ‚úÖ Track changes over time
- ‚úÖ Stop waving your hands in descriptions

**Next steps:**
1. Update `inventory.ini` with your hosts
2. Run your first snapshot
3. Update `AGENTS.md` with the output
4. Use it in your next AI conversation

**No more:** "I have like... 3 or 4 Proxmox hosts? Maybe 5? One's a Dell with GPUs I think..."

**Now:** "See AGENTS.md - I have socrates (Dell R730, 24 vCPU, 128GB RAM, 2x GPU, 64% utilization, uptime 3 days)..."

üé§ *Mic drop* üé§



================================================
FILE: examples/AGENTS.md.example
================================================
# AGENTS.md - Homelab Infrastructure Snapshot

**Last Updated:** 2025-12-05 (Post-Thanksgiving Rebuild)
**Snapshot Level:** Baseball Card + Context
**Status:** Mid-Rebuild

---

## üìç Current Infrastructure State

**What's Happening:** Mid-way through "wipe everything and rebuild for the last time" project. Started right before Thanksgiving, now it's the Friday after.

**Major Changes Since Last Stable:**
- ‚ùå **rseau removed** - Proxmox host is gone
- ‚úÖ **Fan control moved** - Now running on rawls instead of rseau
- üîÑ **MS-01 nodes being added** - 3x Minisforum MS-01 servers, stacked, being configured now
- ‚è≥ **Network reset NOT started** - Planning docs are from Nov 22, but actual reset hasn't happened yet
- üìç **Currently in NYC apartment** - 12U server rack setup

**What This Means for AI Agents:**
- Don't assume planning docs reflect current reality
- Some IPs may not be assigned yet
- Network is still in "old" configuration
- Container locations may have shifted

---

## üèóÔ∏è Infrastructure Inventory

### Proxmox Hosts (VLAN 3 - Lab Management)

#### socrates (10.203.3.42)
- **Role:** Main compute server
- **Hardware:** Dell R730, Dual Xeon CPUs, 2x GPUs
- **Purpose:** AI workloads (Ollama, OpenWebUI)
- **iDRAC:** 10.203.3.8
- **Critical Note:** Requires fan control container or it's **LOUD AS HELL**
- **Status:** Online, fan control now handled by rawls

#### rawls (10.203.3.47)
- **Role:** General purpose Proxmox host
- **Purpose:** Running fan control for socrates, hosting containers
- **Status:** Online
- **Note:** Fan control container moved here from rseau

#### MS-01 Nodes (BEING ADDED)
- **ms01-node1** (10.203.3.11) - **NEW**
- **ms01-node2** (10.203.3.12) - **NEW**
- **ms01-node3** (10.203.3.13) - **NEW**
- **Architecture:** 3-node Proxmox cluster with Thunderbolt ring network
- **Status:** Currently being deployed/configured
- **Purpose:** High-availability cluster for future workloads

#### plato (10.203.3.97 + 10.203.3.98)
- **Role:** Dual-NIC server
- **NICs:** 10.203.3.97 (primary), 10.203.3.98 (secondary)
- **Status:** TBD (check if still in use)

### Storage (VLAN 3)

#### stuffs (10.203.3.99)
- **Role:** Primary NAS
- **Services:**
  - File storage
  - TFTP/PXE boot server (boot file: netboot.xyz.efi)
  - Media stack (Docker containers: Jellyfin, Sonarr, Radarr, Prowlarr, SABnzbd, Overseerr)
- **Status:** Online
- **Note:** Essential for network boots and media services

### Network Infrastructure (VLAN 1)

#### UDM-Pro (10.203.1.1)
- **Role:** Primary gateway and UniFi controller
- **Manages:** All VLANs, DHCP, DNS, firewall rules
- **Status:** Online
- **Note:** Network reset planned but not yet executed

#### Switches & APs
- **USW Pro Max 24** - Main 24-port switch
- **USW Flex Mini** (x2) - Small switches for edge locations
- **U6+ AP** - WiFi 6 access point
- **Status:** All managed via UDM-Pro

### Production Services (VLAN 4 - Planned)

**Note:** Most of these are PLANNED but not yet deployed. Network reset needs to happen first.

#### Identity & Access
- **Authentik** (10.203.4.2) - SSO/Identity provider - **PLANNED**

#### DNS Services
- **Technitium Primary** (10.203.4.3) - DNS for Tailscale network - **PLANNED**
- **Technitium Secondary** (10.203.4.4) - DNS failover - **PLANNED**

#### Networking
- **Tailscale Subnet Router** (10.203.4.5) - LXC for remote access - **PLANNED**

#### Infrastructure as Code
- **GitLab** (10.203.4.15) - Version control, CI/CD - **PLANNED**
- **NetBox** (10.203.4.16) - IPAM, network documentation - **PLANNED**

#### Container Management
- **Dockge** (10.203.4.20) - Docker container UI - **PLANNED** (high priority)

---

## üéØ Essential Services Priority List

When rebuilding, deploy in this order:

1. **Fan Control Container** (rawls) - **CRITICAL** - Dell R730 will be jet-engine loud without it
2. **Dockge** (VLAN 4) - Makes managing other containers easier
3. **Media Stack** (stuffs NAS) - If actively used by household
4. **Ollama** (socrates) - AI backend
5. **OpenWebUI** (socrates) - AI web interface

**Everything else is Phase 2+** (Authentik, DNS cluster, Tailscale router, GitLab, NetBox)

---

## üåê Network Architecture

### VLANs (Planned - Not Yet Implemented)

| VLAN | Name | Subnet | Purpose | Status |
|------|------|--------|---------|--------|
| 1 | Network Hardware | 10.203.1.0/24 | UDM-Pro, switches, APs | Active |
| 2 | Trusted | 10.203.2.0/24 | Personal devices, smart home | Planned |
| 3 | Lab Management (HQ) | 10.203.3.0/24 | Proxmox hosts, iDRAC, NAS | Active |
| 4 | Production | 10.203.4.0/24 | VMs, containers, services | Planned |
| 6 | Staging & Dev | 10.203.6.0/24 | Development, testing | Planned |
| 9 | VPN Network | 10.203.9.0/24 | PIA-routed services | Planned |

### IP Allocation Scheme (Standardized)

```
.1        = Gateway (UDM-Pro)
.2-.10    = Infrastructure (switches, APs, critical services)
.11-.50   = Servers and static services
.51-.100  = Workstations and computers
.101-.150 = IoT and smart home devices
.151-.200 = DHCP pool
.201-.254 = Reserved for future use
```

### Key Network Rules (Planned)

- **VLAN 2 (Trusted) ‚Üí All VLANs:** Allowed (personal devices manage everything)
- **VLAN 4 (Production) ‚Üí VLAN 3 (Management):** DENIED (security: VMs can't access hypervisors)
- **VLAN 6 (Dev) ‚Üí VLAN 4 (Production):** DENIED (isolate test from prod)
- **Exception:** Production services CAN access NAS (10.203.3.99) for backups

---

## üîß Critical Context for AI Agents

### Domain
**doofus.co** (internal only)

### Key Stakeholders
- **Kellen (you):** Primary admin, requires full access from VLAN 2
- **Vickie:** Household user on VLAN 2, should work without technical knowledge

### Deployment Philosophy
- **Switch-level VLANs** (not Proxmox SDN) - Simpler, more reliable
- **LXC for infrastructure services** (DNS, Tailscale router)
- **Docker for applications** (media stack, AI services)
- **GitLab as primary source of truth** (future) ‚Üí NetBox (IPAM) ‚Üí UniFi (active state)

### Architecture Decisions Rationale

**Why separate VLAN 3 and VLAN 4?**
- Security: If a VM is compromised, it can't directly access hypervisors
- Best practice for homelabs in 2025

**Why Tailscale subnet router in LXC?**
- Easier to update/rebuild than installing on Proxmox hosts
- If it breaks, doesn't affect hypervisor stability
- Still provides full network access via subnet routing

**Why only 5 essential containers?**
- Repository used to track 31+ containers
- Most were old, experimental, or ephemeral
- Clean slate approach: only rebuild what's actually needed

---

## üìö Important Documentation Locations

### Planning Documents (TGR-Vault/)
- **Architecture Decisions Summary.md** - Key design choices and rationale
- **Network Reset Procedure.md** - Step-by-step reset guide (NOT YET EXECUTED)
- **IP Allocation Plan.md** - Complete IP assignments
- **Firewall Rules Configuration.md** - Inter-VLAN rules
- **DHCP Reservations - Final List.md** - Static IP assignments
- **Essential Containers List.md** - Priority rebuild list

### Configuration Files
- **networks.json** - UniFi network config export
- **clients.json** - Device inventory with MACs
- **unifi_dhcp_reservations.json** - DHCP reservation config
- **IP_Allocation_Plan.csv** - Spreadsheet format

### Deployment Scripts
- **quick-dockge-lxc-setup.sh** - Deploy Dockge on Proxmox
- **quick-media-stack-stuffs.sh** - Deploy media stack on NAS
- **unifi-netbox-bidirectional-sync.py** - Sync UniFi ‚áÑ NetBox

---

## üö® Known Issues & Gotchas

### Fan Control Container is CRITICAL
- Dell R730 (socrates) is **extremely loud** without fan control
- Container: `Dell_iDRAC_fan_controller` (tigerblue77/dell_idrac_fan_controller:latest)
- Currently on rawls (was on rseau before removal)
- Monitors iDRAC at 10.203.3.8
- Checks temps every minute, ramps fans as needed
- **Deploy this FIRST after any reset**

### Docker Container MACs Change
- Docker containers get MAC addresses starting with `bc:24:11:*`
- These MACs are **dynamically assigned** and change on container recreate
- Don't rely on DHCP reservations for Docker containers
- Use DHCP pool or manually assign IPs in docker-compose

### UniFi Post-Reset IP
- After factory reset, UDM-Pro returns to **192.168.1.1**
- You'll need to change your laptop to static IP 192.168.1.100 to access it
- After initial config, it moves to 10.203.1.1

### rseau is GONE
- If you see references to rseau in old docs, it's been removed
- Fan control moved to rawls
- Technitium DNS containers need new home (likely MS-01 nodes)

---

## üéØ Current Next Steps

1. **Finish MS-01 node deployment** - Get 3-node cluster online
2. **Execute network reset** - Follow Network Reset Procedure.md
3. **Deploy fan control** - FIRST container to deploy post-reset
4. **Deploy Dockge** - Makes other deployments easier
5. **Test network connectivity** - Verify all VLANs routing correctly
6. **Deploy essential containers** - Media stack, Ollama, OpenWebUI

---

## üîÑ How to Update This File

### Manual Updates
Just edit this file directly with current state.

### Automated Snapshots (Ansible)

**Baseball Card Level** (quick facts):
```bash
ansible-playbook -i inventory.ini playbook-baseball-card.yml
# Output: ./snapshots/baseball-card-YYYY-MM-DD.md
```

**Work Friend Level** (detailed info):
```bash
ansible-playbook -i inventory.ini playbook-work-friend.yml
# Output: ./snapshots/work-friend-YYYY-MM-DD.md
```

**Copy snapshot content into this file** to update infrastructure details.

### When to Update
- After major changes (new hosts, service deployments)
- After network reset
- Weekly during rebuild phase
- Before starting conversations with AI agents about infrastructure

---

## üí¨ How AI Agents Should Use This File

### For Planning Discussions
- **Reference current state** - Don't assume docs match reality
- **Ask about unknowns** - If status is unclear, ask instead of assuming
- **Respect phase boundaries** - Don't suggest Phase 2 work if Phase 1 isn't done

### For Troubleshooting
- **Check "Known Issues"** - Common problems are documented
- **Verify host exists** - rseau is gone, MS-01 nodes are new
- **Confirm services running** - Many planned services aren't deployed yet

### For Configuration Changes
- **Follow IP allocation scheme** - Respect the .1-.254 ranges
- **Consider VLAN placement** - Infrastructure vs production services
- **Update this file** - After making changes, update relevant sections

### Example Good Questions
‚úÖ "I see socrates is at 10.203.3.42. Is it currently online?"
‚úÖ "The docs mention Authentik at 10.203.4.2, but it's marked PLANNED. Should we deploy it?"
‚úÖ "MS-01 nodes are listed as 'being added'. What's their current status?"

### Example Bad Assumptions
‚ùå "Let me SSH to rseau to check..." (rseau is gone!)
‚ùå "Since the network reset is documented, all VLANs are configured..." (reset hasn't happened!)
‚ùå "I'll configure the Technitium DNS cluster on rseau..." (rseau is gone!)

---

## üìä Quick Reference Card

**Gateway:** 10.203.1.1 (UDM-Pro)
**DNS:** 1.1.1.1, 8.8.8.8 (currently), 10.203.4.3/4 (planned Technitium)
**Domain:** doofus.co
**NAS:** 10.203.3.99 (stuffs)
**Main Compute:** 10.203.3.42 (socrates, Dell R730)
**iDRAC:** 10.203.3.8 (socrates management)

**Active Proxmox Hosts:** socrates, rawls, ms01-node1/2/3 (new), plato
**Removed Hosts:** rseau (gone as of Thanksgiving 2024)

**Critical First Deploy:** Fan control container (on rawls)
**Current Phase:** Network rebuild in progress, mid-Thanksgiving cleanup

---

*This file is the "baseball card" for AI agents working on this homelab.*
*Keep it updated as the infrastructure evolves.*
*Next major update: After network reset execution.*



================================================
FILE: playbooks/inventory.example.ini
================================================
# Homelab Infrastructure Inventory
# Used for automated snapshots and status checks
# Last updated: 2025-12-05 (post-Thanksgiving rebuild)

# ==============================================================================
# PROXMOX HOSTS (VLAN 3 - Lab Management)
# ==============================================================================

[proxmox_hosts]
# Dell R730 - Main compute with 2x GPUs for AI workloads
socrates ansible_host=10.203.3.42 ansible_user=root
  idrac_ip=10.203.3.8
  role=compute
  gpus=2
  cpu_model="Dual Xeon"
  notes="AI workloads, requires fan control container"

# Mini PC - General purpose Proxmox host
rawls ansible_host=10.203.3.47 ansible_user=root
  role=general
  notes="Fan control container moved here from rseau"

# Minisforum MS-01 nodes (NEW - being added now)
ms01-node1 ansible_host=10.203.3.11 ansible_user=root
  role=cluster
  notes="Part of 3-node Thunderbolt ring cluster"

ms01-node2 ansible_host=10.203.3.12 ansible_user=root
  role=cluster
  notes="Part of 3-node Thunderbolt ring cluster"

ms01-node3 ansible_host=10.203.3.13 ansible_user=root
  role=cluster
  notes="Part of 3-node Thunderbolt ring cluster"

# Dual-NIC server
plato ansible_host=10.203.3.97 ansible_user=root
  nic2_ip=10.203.3.98
  role=general
  notes="Dual-NIC configuration"

[proxmox_hosts:vars]
ansible_python_interpreter=/usr/bin/python3
ansible_ssh_common_args='-o StrictHostKeyChecking=no'

# ==============================================================================
# NETWORK INFRASTRUCTURE (VLAN 1 - Network Hardware)
# ==============================================================================

[network_devices]
# UniFi Dream Machine Pro - Primary gateway
udm-pro ansible_host=10.203.1.1
  device_type=gateway
  role=router
  notes="Primary gateway for all VLANs"

# Switches - managed via UniFi, not directly accessible via SSH
# usw-pro-max-24 ansible_host=10.203.1.2
# usw-flex-mini-1 ansible_host=10.203.1.3
# usw-flex-mini-2 ansible_host=10.203.1.4

# Access Points
# u6-plus-ap ansible_host=10.203.1.10

[network_devices:vars]
ansible_connection=local
# Network devices queried via UniFi API, not direct SSH

# ==============================================================================
# STORAGE (VLAN 3 - Lab Management)
# ==============================================================================

[storage]
# Primary NAS - also TFTP/PXE boot server
stuffs ansible_host=10.203.3.99 ansible_user=kellen ansible_become=yes
  role=nas
  services=["storage", "pxe_boot", "tftp", "media_stack"]
  tftp_server=yes
  boot_file="netboot.xyz.efi"
  notes="Primary NAS, runs media stack docker containers"

[storage:vars]
ansible_python_interpreter=/usr/bin/python3

# ==============================================================================
# PRODUCTION SERVICES (VLAN 4 - Production)
# ==============================================================================

[production_services]
# LXC containers and VMs running production services
# Note: Most are containers, IPs may be dynamic if not reserved

# Identity & Access
authentik ansible_host=10.203.4.2 ansible_user=root
  service_type=lxc
  role=identity
  description="SSO and identity provider"

# DNS Services (Technitium cluster)
dns-primary ansible_host=10.203.4.3 ansible_user=root
  service_type=lxc
  role=dns
  description="Technitium DNS primary node"

dns-secondary ansible_host=10.203.4.4 ansible_user=root
  service_type=lxc
  role=dns
  description="Technitium DNS secondary node"

# Networking
tailscale-subnet-router ansible_host=10.203.4.5 ansible_user=root
  service_type=lxc
  role=networking
  description="Tailscale subnet router for remote access"

# Infrastructure as Code
gitlab ansible_host=10.203.4.15 ansible_user=root
  service_type=lxc
  role=iac
  description="GitLab for version control and CI/CD"

netbox ansible_host=10.203.4.16 ansible_user=root
  service_type=lxc
  role=ipam
  description="NetBox for IPAM and network documentation"

# Container Management
dockge ansible_host=10.203.4.20 ansible_user=root
  service_type=lxc
  role=container_mgmt
  description="Dockge container management UI"

[production_services:vars]
ansible_python_interpreter=/usr/bin/python3
ansible_ssh_common_args='-o StrictHostKeyChecking=no'

# ==============================================================================
# DOCKER CONTAINERS (Various hosts)
# ==============================================================================

[docker_containers]
# These are Docker containers, not directly SSH-able
# Queried via Docker API on their respective hosts

# Fan control (on rawls now, was on rseau)
# fan-control ansible_host=10.203.3.47
#   container_name=Dell_iDRAC_fan_controller
#   docker_host=rawls
#   description="Controls Dell R730 fan speed"

# Media stack (on stuffs NAS)
# media-stack ansible_host=10.203.3.99
#   docker_host=stuffs
#   services=["jellyfin", "sonarr", "radarr", "prowlarr", "sabnzbd", "overseerr"]

# AI Services (on socrates)
# ollama ansible_host=10.203.4.41
#   docker_host=socrates
#   description="LLM backend"

# openwebui ansible_host=10.203.4.42
#   docker_host=socrates
#   description="Web UI for Ollama"

[docker_containers:vars]
ansible_connection=local

# ==============================================================================
# STAGING & DEV (VLAN 6 - Staging & Dev)
# ==============================================================================

[staging]
# Development and testing VMs/containers
# Add as needed

[staging:vars]
ansible_python_interpreter=/usr/bin/python3

# ==============================================================================
# GROUP DEFINITIONS
# ==============================================================================

[all_infrastructure:children]
proxmox_hosts
network_devices
storage
production_services
staging

[ssh_accessible:children]
proxmox_hosts
storage
production_services
staging

[api_accessible:children]
network_devices
docker_containers

# ==============================================================================
# GLOBAL VARIABLES
# ==============================================================================

[all:vars]
# Ansible connection defaults
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
ansible_connection_timeout=10

# Domain
domain=doofus.co

# VLANs
vlan_network_hardware=1
vlan_trusted=2
vlan_lab_management=3
vlan_production=4
vlan_staging=6
vlan_vpn=9

# Network subnets
subnet_vlan1=10.203.1.0/24
subnet_vlan2=10.203.2.0/24
subnet_vlan3=10.203.3.0/24
subnet_vlan4=10.203.4.0/24
subnet_vlan6=10.203.6.0/24
subnet_vlan9=10.203.9.0/24

# Gateway
gateway=10.203.1.1

# DNS servers
dns_primary=1.1.1.1
dns_secondary=8.8.8.8

# Snapshot metadata
snapshot_version=1.0
last_updated=2025-12-05
infrastructure_state=post_thanksgiving_rebuild
notes="Mid-rebuild: rseau removed, MS-01 nodes being added, network reset not yet started"



================================================
FILE: playbooks/playbook-baseball-card.yml
================================================
---
# Baseball Card Level Snapshot
# Quick essential facts about each host - designed for AGENTS.md
# Output: Concise, AI-agent-friendly markdown

- name: Homelab Baseball Card Snapshot
  hosts: ssh_accessible
  gather_facts: yes
  ignore_errors: yes
  any_errors_fatal: no

  tasks:
    - name: Gather system facts
      ansible.builtin.setup:
        gather_subset:
          - '!all'
          - '!min'
          - network
          - hardware
          - virtual
      register: facts_output

    - name: Check if host is reachable
      ansible.builtin.ping:
      register: ping_result

    - name: Get uptime
      ansible.builtin.command: uptime -p
      register: uptime_output
      changed_when: false
      failed_when: false

    - name: Get disk usage summary
      ansible.builtin.shell: df -h / | tail -1 | awk '{print $5}'
      register: disk_usage
      changed_when: false
      failed_when: false

    - name: Get memory usage
      ansible.builtin.shell: free -h | grep Mem | awk '{print $3 "/" $2}'
      register: memory_usage
      changed_when: false
      failed_when: false

    - name: Check if Proxmox
      ansible.builtin.stat:
        path: /usr/bin/pvesh
      register: is_proxmox

    - name: Get Proxmox version (if applicable)
      ansible.builtin.command: pveversion
      register: proxmox_version
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: List running LXC containers (Proxmox)
      ansible.builtin.shell: pct list | tail -n +2 | awk '{print $3}' | tr '\n' ', ' | sed 's/,$//'
      register: lxc_containers
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: List running VMs (Proxmox)
      ansible.builtin.shell: qm list | tail -n +2 | awk '{print $2}' | tr '\n' ', ' | sed 's/,$//'
      register: running_vms
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: Check for Docker
      ansible.builtin.stat:
        path: /usr/bin/docker
      register: has_docker

    - name: List running Docker containers
      ansible.builtin.shell: docker ps --format '{{.Names}}' | tr '\n' ', ' | sed 's/,$//'
      register: docker_containers
      when: has_docker.stat.exists
      changed_when: false
      failed_when: false

    - name: Get listening ports
      ansible.builtin.shell: ss -tlnp | grep LISTEN | awk '{print $4}' | awk -F: '{print $NF}' | sort -u | tr '\n' ', ' | sed 's/,$//'
      register: listening_ports
      changed_when: false
      failed_when: false

- name: Generate Baseball Card Report
  hosts: localhost
  gather_facts: no
  run_once: yes

  tasks:
    - name: Create report directory
      ansible.builtin.file:
        path: ./snapshots
        state: directory
        mode: '0755'

    - name: Generate baseball card markdown
      ansible.builtin.template:
        src: templates/baseball-card.md.j2
        dest: "./snapshots/baseball-card-{{ ansible_date_time.date }}.md"
      delegate_to: localhost

    - name: Display report location
      ansible.builtin.debug:
        msg: "Baseball card snapshot saved to: ./snapshots/baseball-card-{{ ansible_date_time.date }}.md"



================================================
FILE: playbooks/playbook-work-friend.yml
================================================
---
# Work Friend Level Snapshot
# Extended information - like chatting with a coworker about the infrastructure
# Includes: storage details, recent logs, service health, performance metrics

- name: Homelab Work Friend Snapshot
  hosts: ssh_accessible
  gather_facts: yes
  ignore_errors: yes
  any_errors_fatal: no

  tasks:
    # =========================================================================
    # BASIC FACTS (same as baseball card)
    # =========================================================================

    - name: Gather system facts
      ansible.builtin.setup:
        gather_subset:
          - '!all'
          - network
          - hardware
          - virtual
      register: facts_output

    - name: Check if host is reachable
      ansible.builtin.ping:
      register: ping_result

    - name: Get uptime
      ansible.builtin.command: uptime -p
      register: uptime_output
      changed_when: false
      failed_when: false

    - name: Get load average
      ansible.builtin.shell: uptime | awk -F'load average:' '{print $2}' | xargs
      register: load_average
      changed_when: false
      failed_when: false

    # =========================================================================
    # STORAGE DETAILS
    # =========================================================================

    - name: Get detailed disk usage
      ansible.builtin.shell: df -h | grep -E '^/dev|^Filesystem' | column -t
      register: disk_details
      changed_when: false
      failed_when: false

    - name: Get largest directories (top 10)
      ansible.builtin.shell: du -h / --max-depth=2 2>/dev/null | sort -hr | head -10
      register: large_dirs
      changed_when: false
      failed_when: false
      when: ansible_facts.ansible_system == "Linux"

    - name: Check ZFS pools (if applicable)
      ansible.builtin.shell: zpool list
      register: zfs_pools
      changed_when: false
      failed_when: false

    - name: Check LVM volumes
      ansible.builtin.shell: lvdisplay | grep -E 'LV Path|LV Size' | paste - -
      register: lvm_volumes
      changed_when: false
      failed_when: false

    # =========================================================================
    # MEMORY & PERFORMANCE
    # =========================================================================

    - name: Get detailed memory info
      ansible.builtin.shell: free -h
      register: memory_details
      changed_when: false
      failed_when: false

    - name: Get top 5 processes by CPU
      ansible.builtin.shell: ps aux --sort=-%cpu | head -6
      register: top_cpu_processes
      changed_when: false
      failed_when: false

    - name: Get top 5 processes by memory
      ansible.builtin.shell: ps aux --sort=-%mem | head -6
      register: top_mem_processes
      changed_when: false
      failed_when: false

    # =========================================================================
    # NETWORK INFO
    # =========================================================================

    - name: Get network interface details
      ansible.builtin.shell: ip -brief addr show
      register: network_interfaces
      changed_when: false
      failed_when: false

    - name: Get listening services with process names
      ansible.builtin.shell: ss -tlnp | grep LISTEN
      register: listening_services
      changed_when: false
      failed_when: false

    - name: Get established connections count
      ansible.builtin.shell: ss -tan | grep ESTAB | wc -l
      register: established_connections
      changed_when: false
      failed_when: false

    # =========================================================================
    # RECENT LOGS
    # =========================================================================

    - name: Get last 50 syslog entries
      ansible.builtin.shell: journalctl -n 50 --no-pager
      register: recent_logs
      changed_when: false
      failed_when: false

    - name: Get error/warning count in last 100 log entries
      ansible.builtin.shell: journalctl -n 100 --no-pager | grep -iE 'error|warn|fail' | wc -l
      register: error_count
      changed_when: false
      failed_when: false

    - name: Get failed systemd services
      ansible.builtin.shell: systemctl list-units --state=failed --no-pager
      register: failed_services
      changed_when: false
      failed_when: false

    # =========================================================================
    # PROXMOX SPECIFIC
    # =========================================================================

    - name: Check if Proxmox
      ansible.builtin.stat:
        path: /usr/bin/pvesh
      register: is_proxmox

    - name: Get Proxmox version
      ansible.builtin.command: pveversion
      register: proxmox_version
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: Get detailed LXC container list
      ansible.builtin.shell: pct list
      register: lxc_list
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: Get detailed VM list
      ansible.builtin.shell: qm list
      register: vm_list
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: Get Proxmox storage status
      ansible.builtin.shell: pvesm status
      register: proxmox_storage
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    - name: Get Proxmox cluster status
      ansible.builtin.shell: pvecm status 2>&1 || echo "Not in a cluster"
      register: cluster_status
      when: is_proxmox.stat.exists
      changed_when: false
      failed_when: false

    # =========================================================================
    # DOCKER SPECIFIC
    # =========================================================================

    - name: Check for Docker
      ansible.builtin.stat:
        path: /usr/bin/docker
      register: has_docker

    - name: Get Docker version
      ansible.builtin.command: docker --version
      register: docker_version
      when: has_docker.stat.exists
      changed_when: false
      failed_when: false

    - name: Get detailed Docker container list
      ansible.builtin.shell: docker ps -a --format 'table {{.Names}}\t{{.Status}}\t{{.Image}}'
      register: docker_containers_detailed
      when: has_docker.stat.exists
      changed_when: false
      failed_when: false

    - name: Get Docker disk usage
      ansible.builtin.command: docker system df
      register: docker_disk_usage
      when: has_docker.stat.exists
      changed_when: false
      failed_when: false

    - name: Get Docker network list
      ansible.builtin.command: docker network ls
      register: docker_networks
      when: has_docker.stat.exists
      changed_when: false
      failed_when: false

    # =========================================================================
    # SECURITY & UPDATES
    # =========================================================================

    - name: Check for available updates (Debian/Ubuntu)
      ansible.builtin.shell: apt list --upgradable 2>/dev/null | wc -l
      register: available_updates
      when: ansible_facts.ansible_os_family == "Debian"
      changed_when: false
      failed_when: false

    - name: Get last apt update time
      ansible.builtin.stat:
        path: /var/cache/apt/pkgcache.bin
      register: apt_cache
      when: ansible_facts.ansible_os_family == "Debian"

    - name: Check SSH login attempts (last 20)
      ansible.builtin.shell: journalctl -u ssh -n 20 --no-pager | grep -i 'accepted\|failed' || echo "No recent SSH events"
      register: ssh_logins
      changed_when: false
      failed_when: false

    # =========================================================================
    # TEMPERATURE & HARDWARE (if available)
    # =========================================================================

    - name: Check for sensors package
      ansible.builtin.shell: command -v sensors
      register: has_sensors
      changed_when: false
      failed_when: false

    - name: Get temperature readings
      ansible.builtin.shell: sensors 2>/dev/null || echo "Sensors not available"
      register: temperature_readings
      when: has_sensors.rc == 0
      changed_when: false
      failed_when: false

    - name: Get iDRAC info (if Dell server)
      ansible.builtin.shell: ipmitool sdr type temperature 2>/dev/null || echo "iDRAC/IPMI not available"
      register: idrac_temps
      changed_when: false
      failed_when: false

- name: Generate Work Friend Report
  hosts: localhost
  gather_facts: no
  run_once: yes

  tasks:
    - name: Create report directory
      ansible.builtin.file:
        path: ./snapshots
        state: directory
        mode: '0755'

    - name: Generate work friend markdown
      ansible.builtin.template:
        src: templates/work-friend.md.j2
        dest: "./snapshots/work-friend-{{ ansible_date_time.date }}.md"
      delegate_to: localhost

    - name: Display report location
      ansible.builtin.debug:
        msg: "Work friend snapshot saved to: ./snapshots/work-friend-{{ ansible_date_time.date }}.md"



================================================
FILE: templates/baseball-card.md.j2
================================================
# Homelab Baseball Card Snapshot
**Generated:** {{ ansible_date_time.iso8601 }}
**Snapshot Type:** Baseball Card (Essential Facts)

---

## üìä Quick Stats

| Metric | Value |
|--------|-------|
| Total Hosts | {{ groups['ssh_accessible'] | length }} |
| Proxmox Hosts | {{ groups['proxmox_hosts'] | length }} |
| Production Services | {{ groups['production_services'] | length }} |
| Storage Hosts | {{ groups['storage'] | length }} |

---

{% for host in groups['ssh_accessible'] %}
{% set facts = hostvars[host] %}
{% if facts.ping_result is defined and facts.ping_result.ping is defined %}
## üñ•Ô∏è {{ host }}

**Status:** ‚úÖ Online
**IP Address:** {{ facts.ansible_host }}
**Role:** {{ facts.role | default('unknown') }}
{% if facts.ansible_facts is defined %}
**OS:** {{ facts.ansible_facts.ansible_distribution | default('Unknown') }} {{ facts.ansible_facts.ansible_distribution_version | default('') }}
**Hostname:** {{ facts.ansible_facts.ansible_hostname | default(host) }}
**Architecture:** {{ facts.ansible_facts.ansible_architecture | default('unknown') }}
{% endif %}
{% if facts.uptime_output is defined and facts.uptime_output.stdout is defined %}
**Uptime:** {{ facts.uptime_output.stdout }}
{% endif %}
{% if facts.ansible_facts is defined and facts.ansible_facts.ansible_processor_vcpus is defined %}
**CPU:** {{ facts.ansible_facts.ansible_processor_vcpus }} vCPUs
{% endif %}
{% if facts.ansible_facts is defined and facts.ansible_facts.ansible_memtotal_mb is defined %}
**RAM:** {{ (facts.ansible_facts.ansible_memtotal_mb / 1024) | round(1) }} GB{% if facts.memory_usage.stdout is defined %} ({{ facts.memory_usage.stdout }} used){% endif %}
{% endif %}
{% if facts.disk_usage is defined and facts.disk_usage.stdout is defined %}
**Disk Usage:** {{ facts.disk_usage.stdout }}
{% endif %}

{% if facts.is_proxmox is defined and facts.is_proxmox.stat.exists %}
**Type:** Proxmox Host
{% if facts.proxmox_version is defined and facts.proxmox_version.stdout is defined %}
**Proxmox Version:** {{ facts.proxmox_version.stdout }}
{% endif %}
{% if facts.lxc_containers is defined and facts.lxc_containers.stdout is defined and facts.lxc_containers.stdout | length > 0 %}
**LXC Containers:** {{ facts.lxc_containers.stdout }}
{% endif %}
{% if facts.running_vms is defined and facts.running_vms.stdout is defined and facts.running_vms.stdout | length > 0 %}
**VMs:** {{ facts.running_vms.stdout }}
{% endif %}
{% endif %}

{% if facts.has_docker is defined and facts.has_docker.stat.exists %}
**Docker:** Installed
{% if facts.docker_containers is defined and facts.docker_containers.stdout is defined and facts.docker_containers.stdout | length > 0 %}
**Running Containers:** {{ facts.docker_containers.stdout }}
{% endif %}
{% endif %}

{% if facts.listening_ports is defined and facts.listening_ports.stdout is defined and facts.listening_ports.stdout | length > 0 %}
**Listening Ports:** {{ facts.listening_ports.stdout }}
{% endif %}

{% if facts.description is defined %}
**Description:** {{ facts.description }}
{% endif %}
{% if facts.notes is defined %}
**Notes:** {{ facts.notes }}
{% endif %}

---

{% else %}
## ‚ùå {{ host }}

**Status:** Offline or Unreachable
**IP Address:** {{ facts.ansible_host }}
**Last Known Role:** {{ facts.role | default('unknown') }}

---

{% endif %}
{% endfor %}

## üìù Infrastructure Notes

**Current State:** {{ hostvars['localhost'].infrastructure_state | default('Post-Thanksgiving rebuild in progress') }}

**Recent Changes:**
- rseau (Proxmox host) - **REMOVED**
- MS-01 nodes (3x) - **BEING ADDED**
- Fan control container - **MOVED TO rawls**
- Network reset - **NOT YET STARTED**

**Next Steps:**
- Complete MS-01 node deployment
- Execute network reset procedure
- Deploy essential containers (fan control, Dockge, media stack)
- Configure Tailscale subnet router

---

*Generated by Ansible Baseball Card Playbook*
*For AI agents: This snapshot provides essential infrastructure facts.*
*Update frequency: Run on-demand or scheduled (recommended: weekly)*



================================================
FILE: templates/work-friend.md.j2
================================================
# Homelab Work Friend Snapshot
**Generated:** {{ ansible_date_time.iso8601 }}
**Snapshot Type:** Work Friend (Extended Details)

This snapshot is like catching up with a coworker - more detail, context, and "how are things really going?"

---

## üìä Infrastructure Overview

| Metric | Value |
|--------|-------|
| Total Hosts | {{ groups['ssh_accessible'] | length }} |
| Proxmox Hosts | {{ groups['proxmox_hosts'] | length }} |
| Production Services | {{ groups['production_services'] | length }} |
| Storage Hosts | {{ groups['storage'] | length }} |
| Snapshot Time | {{ ansible_date_time.time }} UTC |

---

{% for host in groups['ssh_accessible'] %}
{% set facts = hostvars[host] %}
{% if facts.ping_result is defined and facts.ping_result.ping is defined %}
# üñ•Ô∏è {{ host | upper }}

## Basic Info
- **Status:** ‚úÖ Online
- **IP Address:** {{ facts.ansible_host }}
- **Role:** {{ facts.role | default('unknown') }}
{% if facts.ansible_facts is defined %}
- **OS:** {{ facts.ansible_facts.ansible_distribution | default('Unknown') }} {{ facts.ansible_facts.ansible_distribution_version | default('') }}
- **Kernel:** {{ facts.ansible_facts.ansible_kernel | default('unknown') }}
- **Hostname:** {{ facts.ansible_facts.ansible_hostname | default(host) }}
- **Architecture:** {{ facts.ansible_facts.ansible_architecture | default('unknown') }}
{% endif %}
{% if facts.uptime_output is defined and facts.uptime_output.stdout is defined %}
- **Uptime:** {{ facts.uptime_output.stdout }}
{% endif %}
{% if facts.load_average is defined and facts.load_average.stdout is defined %}
- **Load Average:** {{ facts.load_average.stdout }}
{% endif %}

## Hardware Resources
{% if facts.ansible_facts is defined %}
- **CPU Cores:** {{ facts.ansible_facts.ansible_processor_vcpus | default('unknown') }} vCPUs
- **CPU Model:** {{ facts.ansible_facts.ansible_processor[2] | default('unknown') if facts.ansible_facts.ansible_processor is defined else 'unknown' }}
- **Total RAM:** {{ (facts.ansible_facts.ansible_memtotal_mb / 1024) | round(1) }} GB
{% endif %}

{% if facts.memory_details is defined and facts.memory_details.stdout is defined %}
### Memory Details
```
{{ facts.memory_details.stdout }}
```
{% endif %}

## Storage
{% if facts.disk_details is defined and facts.disk_details.stdout is defined %}
```
{{ facts.disk_details.stdout }}
```
{% endif %}

{% if facts.zfs_pools is defined and facts.zfs_pools.rc == 0 %}
### ZFS Pools
```
{{ facts.zfs_pools.stdout }}
```
{% endif %}

{% if facts.lvm_volumes is defined and facts.lvm_volumes.rc == 0 and facts.lvm_volumes.stdout | length > 0 %}
### LVM Volumes
```
{{ facts.lvm_volumes.stdout }}
```
{% endif %}

{% if facts.large_dirs is defined and facts.large_dirs.stdout is defined %}
### Top 10 Largest Directories
```
{{ facts.large_dirs.stdout }}
```
{% endif %}

## Network
{% if facts.network_interfaces is defined and facts.network_interfaces.stdout is defined %}
```
{{ facts.network_interfaces.stdout }}
```
{% endif %}

{% if facts.listening_services is defined and facts.listening_services.stdout is defined %}
### Listening Services
```
{{ facts.listening_services.stdout }}
```
{% endif %}

{% if facts.established_connections is defined and facts.established_connections.stdout is defined %}
- **Active Connections:** {{ facts.established_connections.stdout }}
{% endif %}

## Performance
{% if facts.top_cpu_processes is defined and facts.top_cpu_processes.stdout is defined %}
### Top 5 Processes by CPU
```
{{ facts.top_cpu_processes.stdout }}
```
{% endif %}

{% if facts.top_mem_processes is defined and facts.top_mem_processes.stdout is defined %}
### Top 5 Processes by Memory
```
{{ facts.top_mem_processes.stdout }}
```
{% endif %}

{% if facts.temperature_readings is defined and facts.temperature_readings.stdout is defined and 'not available' not in facts.temperature_readings.stdout %}
### Temperature Readings
```
{{ facts.temperature_readings.stdout }}
```
{% endif %}

{% if facts.idrac_temps is defined and 'not available' not in facts.idrac_temps.stdout %}
### iDRAC/IPMI Temperatures
```
{{ facts.idrac_temps.stdout }}
```
{% endif %}

{% if facts.is_proxmox is defined and facts.is_proxmox.stat.exists %}
## Proxmox Details
{% if facts.proxmox_version is defined and facts.proxmox_version.stdout is defined %}
- **Version:** {{ facts.proxmox_version.stdout }}
{% endif %}

{% if facts.cluster_status is defined and facts.cluster_status.stdout is defined %}
### Cluster Status
```
{{ facts.cluster_status.stdout }}
```
{% endif %}

{% if facts.lxc_list is defined and facts.lxc_list.stdout is defined %}
### LXC Containers
```
{{ facts.lxc_list.stdout }}
```
{% endif %}

{% if facts.vm_list is defined and facts.vm_list.stdout is defined %}
### Virtual Machines
```
{{ facts.vm_list.stdout }}
```
{% endif %}

{% if facts.proxmox_storage is defined and facts.proxmox_storage.stdout is defined %}
### Storage Status
```
{{ facts.proxmox_storage.stdout }}
```
{% endif %}
{% endif %}

{% if facts.has_docker is defined and facts.has_docker.stat.exists %}
## Docker Details
{% if facts.docker_version is defined and facts.docker_version.stdout is defined %}
- **Version:** {{ facts.docker_version.stdout }}
{% endif %}

{% if facts.docker_containers_detailed is defined and facts.docker_containers_detailed.stdout is defined %}
### Containers
```
{{ facts.docker_containers_detailed.stdout }}
```
{% endif %}

{% if facts.docker_disk_usage is defined and facts.docker_disk_usage.stdout is defined %}
### Disk Usage
```
{{ facts.docker_disk_usage.stdout }}
```
{% endif %}

{% if facts.docker_networks is defined and facts.docker_networks.stdout is defined %}
### Networks
```
{{ facts.docker_networks.stdout }}
```
{% endif %}
{% endif %}

## Health & Logs
{% if facts.error_count is defined and facts.error_count.stdout is defined %}
- **Recent Errors/Warnings:** {{ facts.error_count.stdout }} in last 100 log entries
{% endif %}

{% if facts.failed_services is defined and facts.failed_services.stdout is defined and facts.failed_services.stdout | length > 10 %}
### Failed Services
```
{{ facts.failed_services.stdout }}
```
{% endif %}

{% if facts.recent_logs is defined and facts.recent_logs.stdout is defined %}
### Last 50 Log Entries
```
{{ facts.recent_logs.stdout }}
```
{% endif %}

{% if facts.ssh_logins is defined and facts.ssh_logins.stdout is defined %}
### Recent SSH Activity
```
{{ facts.ssh_logins.stdout }}
```
{% endif %}

## Updates & Security
{% if facts.available_updates is defined and facts.available_updates.stdout is defined %}
- **Available Updates:** {{ facts.available_updates.stdout }} packages
{% endif %}

{% if facts.apt_cache is defined and facts.apt_cache.stat.exists %}
- **Last APT Update:** {{ facts.apt_cache.stat.mtime | default('unknown') }}
{% endif %}

## Notes
{% if facts.description is defined %}
**Description:** {{ facts.description }}
{% endif %}
{% if facts.notes is defined %}
**Notes:** {{ facts.notes }}
{% endif %}

---

{% else %}
# ‚ùå {{ host | upper }}

**Status:** Offline or Unreachable
**IP Address:** {{ facts.ansible_host }}
**Last Known Role:** {{ facts.role | default('unknown') }}

This host did not respond to ping. Possible reasons:
- Host is powered off
- Network connectivity issue
- SSH service not running
- Firewall blocking access

---

{% endif %}
{% endfor %}

# üìà Aggregate Stats

## Resource Summary
{% set total_vcpus = 0 %}
{% set total_ram_gb = 0 %}
{% set online_hosts = 0 %}
{% for host in groups['ssh_accessible'] %}
{% set facts = hostvars[host] %}
{% if facts.ping_result is defined and facts.ping_result.ping is defined %}
{% set online_hosts = online_hosts + 1 %}
{% if facts.ansible_facts is defined and facts.ansible_facts.ansible_processor_vcpus is defined %}
{% set total_vcpus = total_vcpus + facts.ansible_facts.ansible_processor_vcpus %}
{% endif %}
{% if facts.ansible_facts is defined and facts.ansible_facts.ansible_memtotal_mb is defined %}
{% set total_ram_gb = total_ram_gb + (facts.ansible_facts.ansible_memtotal_mb / 1024) %}
{% endif %}
{% endif %}
{% endfor %}

- **Online Hosts:** {{ online_hosts }} / {{ groups['ssh_accessible'] | length }}
- **Total vCPUs:** {{ total_vcpus }}
- **Total RAM:** {{ total_ram_gb | round(1) }} GB

## Infrastructure Health Summary

{% set total_errors = 0 %}
{% for host in groups['ssh_accessible'] %}
{% set facts = hostvars[host] %}
{% if facts.error_count is defined and facts.error_count.stdout is defined %}
{% set total_errors = total_errors + (facts.error_count.stdout | int) %}
{% endif %}
{% endfor %}

- **Total Recent Errors/Warnings:** {{ total_errors }} across all hosts
- **Overall Status:** {% if total_errors < 10 %}üü¢ Healthy{% elif total_errors < 50 %}üü° Minor Issues{% else %}üî¥ Attention Needed{% endif %}

---

# ü§ñ AI Agent Notes

This is your "work friend" level snapshot - more conversational detail than the baseball card, but not overwhelming. Use this to:

1. **Understand current state** - What's actually running right now
2. **Spot issues** - Failed services, high error counts, disk usage
3. **Performance context** - Load averages, top processes, resource usage
4. **Recent activity** - Logs, SSH logins, changes

**Key Infrastructure Context:**
- **Current State:** Mid-rebuild (post-Thanksgiving)
- **rseau removed:** Fan control moved to rawls
- **MS-01 nodes being added:** 3-node Thunderbolt cluster
- **Network reset pending:** Not yet executed
- **Essential containers:** Fan control, Dockge, media stack (priorities)

**When to re-snapshot:**
- After major changes (new hosts, network reset)
- When performance issues arise
- Weekly for general health monitoring
- Before/after maintenance windows

---

*Generated by Ansible Work Friend Playbook*
*Update frequency: On-demand or weekly recommended*
*Next snapshot: Run after network reset completion*


